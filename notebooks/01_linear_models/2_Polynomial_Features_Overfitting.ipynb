{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Polynomial Features & Overfitting\n",
    "\n",
    "Goal: Understand how feature engineering extends linear models to capture nonlinear patterns, and why this power comes with the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-theory",
   "metadata": {},
   "source": [
    "## The Linearity Constraint\n",
    "\n",
    "Linear regression fits models of the form:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d$$\n",
    "\n",
    "This is **linear in the features** $x_i$. But what if the true relationship is nonlinear?\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "Linear regression is linear in the **parameters** $w_i$, not necessarily in the original inputs. We can create new features that are nonlinear transformations of the inputs:\n",
    "\n",
    "$$\\phi(x) = [1, x, x^2, x^3, \\ldots, x^p]$$\n",
    "\n",
    "Then fit a linear model on these **polynomial features**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\cdots + w_p x^p$$\n",
    "\n",
    "This is still a linear model (linear in $w_i$), but it can capture nonlinear patterns in $x$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polynomial-transform",
   "metadata": {},
   "source": [
    "## Polynomial Feature Transformation\n",
    "\n",
    "### Univariate Case\n",
    "\n",
    "For a single feature $x$, the polynomial transformation of degree $p$ is:\n",
    "\n",
    "$$\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}^{p+1}$$\n",
    "$$\\phi(x) = [1, x, x^2, \\ldots, x^p]^T$$\n",
    "\n",
    "The design matrix becomes:\n",
    "\n",
    "$$\\Phi = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^p \\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^p \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_n & x_n^2 & \\cdots & x_n^p \\end{bmatrix}$$\n",
    "\n",
    "### Multivariate Case\n",
    "\n",
    "For multiple features, we include all polynomial terms up to degree $p$, including **interaction terms**:\n",
    "\n",
    "For $x = [x_1, x_2]$ and degree 2:\n",
    "$$\\phi(x) = [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]$$\n",
    "\n",
    "The number of features grows combinatorially: $\\binom{d + p}{p}$ for $d$ original features and degree $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "poly-transform-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def polynomial_features(x, degree):\n",
    "    \"\"\"\n",
    "    Create polynomial features for univariate input.\n",
    "    \n",
    "    Parameters:\n",
    "        x: array of shape (n,) - input values\n",
    "        degree: int - maximum polynomial degree\n",
    "    \n",
    "    Returns:\n",
    "        Phi: array of shape (n, degree+1) - design matrix\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    Phi = np.zeros((n, degree + 1))\n",
    "    for p in range(degree + 1):\n",
    "        Phi[:, p] = x ** p\n",
    "    return Phi\n",
    "\n",
    "# Example: transform x = [1, 2, 3] with degree 3\n",
    "x_example = np.array([1, 2, 3])\n",
    "Phi_example = polynomial_features(x_example, degree=3)\n",
    "\n",
    "print(\"Original x:\", x_example)\n",
    "print(\"\\nPolynomial features (degree 3):\")\n",
    "print(\"  [1, x, x², x³]\")\n",
    "for i, xi in enumerate(x_example):\n",
    "    print(f\"  x={xi}: {Phi_example[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-polynomials",
   "metadata": {},
   "source": [
    "## Fitting Polynomial Models\n",
    "\n",
    "Once we have the polynomial features, fitting is identical to linear regression:\n",
    "\n",
    "$$\\mathbf{w} = (\\Phi^T\\Phi)^{-1}\\Phi^T y$$\n",
    "\n",
    "The prediction for a new point $x^*$ is:\n",
    "\n",
    "$$\\hat{y}^* = \\phi(x^*)^T \\mathbf{w} = w_0 + w_1 x^* + w_2 (x^*)^2 + \\cdots$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-polynomial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic nonlinear data\n",
    "np.random.seed(42)\n",
    "n = 30\n",
    "x = np.linspace(-3, 3, n)\n",
    "y_true = 0.5 * x**3 - 2 * x**2 + x + 3  # True cubic relationship\n",
    "y = y_true + np.random.randn(n) * 3  # Add noise\n",
    "\n",
    "def fit_polynomial(x, y, degree):\n",
    "    \"\"\"Fit polynomial regression and return weights.\"\"\"\n",
    "    Phi = polynomial_features(x, degree)\n",
    "    w = np.linalg.inv(Phi.T @ Phi) @ Phi.T @ y\n",
    "    return w\n",
    "\n",
    "def predict_polynomial(x, w):\n",
    "    \"\"\"Predict using polynomial weights.\"\"\"\n",
    "    degree = len(w) - 1\n",
    "    Phi = polynomial_features(x, degree)\n",
    "    return Phi @ w\n",
    "\n",
    "# Fit models of different degrees\n",
    "x_plot = np.linspace(-3.5, 3.5, 200)\n",
    "degrees = [1, 3, 9, 15]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot data\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Data',\n",
    "                         marker=dict(size=10, color='blue')))\n",
    "\n",
    "# Plot true function\n",
    "fig.add_trace(go.Scatter(x=x_plot, y=0.5*x_plot**3 - 2*x_plot**2 + x_plot + 3,\n",
    "                         mode='lines', name='True function',\n",
    "                         line=dict(color='black', dash='dash', width=2)))\n",
    "\n",
    "# Plot polynomial fits\n",
    "colors = ['green', 'orange', 'red', 'purple']\n",
    "for deg, color in zip(degrees, colors):\n",
    "    w = fit_polynomial(x, y, deg)\n",
    "    y_pred = predict_polynomial(x_plot, w)\n",
    "    fig.add_trace(go.Scatter(x=x_plot, y=y_pred, mode='lines',\n",
    "                             name=f'Degree {deg}',\n",
    "                             line=dict(color=color, width=2)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Polynomial Regression: Effect of Degree',\n",
    "    xaxis_title='x', yaxis_title='y',\n",
    "    yaxis=dict(range=[-30, 30]),\n",
    "    width=800, height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-capacity",
   "metadata": {},
   "source": [
    "## Model Capacity and Flexibility\n",
    "\n",
    "### What is Capacity?\n",
    "\n",
    "**Model capacity** (or complexity) refers to the variety of functions a model can fit. Higher capacity means the model can represent more complex patterns.\n",
    "\n",
    "For polynomial regression:\n",
    "- **Degree 1** (linear): Can only fit straight lines\n",
    "- **Degree 2** (quadratic): Can fit parabolas\n",
    "- **Degree $n-1$**: Can pass through all $n$ data points exactly!\n",
    "\n",
    "### Degrees of Freedom\n",
    "\n",
    "A degree-$p$ polynomial has $p+1$ parameters (degrees of freedom). With $n$ data points:\n",
    "\n",
    "- If $p + 1 < n$: System is **overdetermined** (more equations than unknowns)\n",
    "- If $p + 1 = n$: System is **exactly determined** (unique solution through all points)\n",
    "- If $p + 1 > n$: System is **underdetermined** (infinitely many solutions)\n",
    "\n",
    "### The Interpolation Trap\n",
    "\n",
    "When $p \\geq n - 1$, we can achieve **zero training error**! The polynomial passes through every data point. But is this good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capacity-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the interpolation trap\n",
    "np.random.seed(42)\n",
    "n_small = 10\n",
    "x_small = np.linspace(-2, 2, n_small)\n",
    "y_small = np.sin(x_small) + np.random.randn(n_small) * 0.3\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Training Data Fit', 'Training MSE vs Degree'))\n",
    "\n",
    "# Left plot: visualize fits\n",
    "x_dense = np.linspace(-2.5, 2.5, 200)\n",
    "for deg in [1, 3, 5, 9]:\n",
    "    w = fit_polynomial(x_small, y_small, deg)\n",
    "    y_pred = predict_polynomial(x_dense, w)\n",
    "    # Clip extreme values for visualization\n",
    "    y_pred = np.clip(y_pred, -5, 5)\n",
    "    fig.add_trace(go.Scatter(x=x_dense, y=y_pred, mode='lines', name=f'Degree {deg}'),\n",
    "                  row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_small, y=y_small, mode='markers', name='Data',\n",
    "                         marker=dict(size=12, color='black')), row=1, col=1)\n",
    "\n",
    "# Right plot: training error vs degree\n",
    "degrees = range(1, n_small)\n",
    "train_errors = []\n",
    "for deg in degrees:\n",
    "    w = fit_polynomial(x_small, y_small, deg)\n",
    "    y_pred = predict_polynomial(x_small, w)\n",
    "    mse = np.mean((y_small - y_pred)**2)\n",
    "    train_errors.append(mse)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(degrees), y=train_errors, mode='lines+markers',\n",
    "                         name='Training MSE', marker=dict(size=10)),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(range=[-3, 3], row=1, col=1)\n",
    "fig.update_xaxes(title_text='x', row=1, col=1)\n",
    "fig.update_yaxes(title_text='y', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Polynomial Degree', row=1, col=2)\n",
    "fig.update_yaxes(title_text='MSE', type='log', row=1, col=2)\n",
    "\n",
    "fig.update_layout(width=1000, height=400, title='Model Capacity: More Degrees = Lower Training Error')\n",
    "fig.show()\n",
    "\n",
    "print(f\"With n={n_small} points, degree {n_small-1} achieves near-zero training error.\")\n",
    "print(f\"Training MSE at degree {n_small-1}: {train_errors[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overfitting-theory",
   "metadata": {},
   "source": [
    "## Overfitting: When Models Memorize Instead of Learn\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Overfitting** occurs when a model learns patterns specific to the training data that don't generalize to new data. The model memorizes noise rather than learning the underlying relationship.\n",
    "\n",
    "### Symptoms of Overfitting\n",
    "\n",
    "1. **Low training error, high test error**: The gap between them is the key signal\n",
    "2. **Large weight magnitudes**: Overfitted polynomials have huge coefficients\n",
    "3. **Oscillating predictions**: Wild swings between data points\n",
    "4. **Sensitivity to small changes**: Adding one point dramatically changes the fit\n",
    "\n",
    "### Why Does Overfitting Happen?\n",
    "\n",
    "Consider fitting a degree-9 polynomial to 10 noisy points:\n",
    "- The model has 10 parameters and 10 data points\n",
    "- It can achieve perfect fit by solving a system of 10 equations with 10 unknowns\n",
    "- But it has learned the **noise**, not just the **signal**\n",
    "\n",
    "### The Mathematical Perspective\n",
    "\n",
    "Observed data: $y = f(x) + \\epsilon$ where $f$ is the true function and $\\epsilon$ is noise.\n",
    "\n",
    "An overfitted model $\\hat{f}$ satisfies:\n",
    "- $\\hat{f}(x_i) \\approx y_i = f(x_i) + \\epsilon_i$ (fits training points)\n",
    "- But $\\hat{f}(x^*) \\neq f(x^*)$ for new points $x^*$ (poor generalization)\n",
    "\n",
    "The model has captured $\\epsilon_i$ (noise) in addition to $f(x_i)$ (signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overfitting-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting with train/test split\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate more data\n",
    "n_total = 50\n",
    "x_all = np.random.uniform(-3, 3, n_total)\n",
    "y_true_all = np.sin(x_all) * 2 + 0.5 * x_all  # True underlying function\n",
    "y_all = y_true_all + np.random.randn(n_total) * 0.5  # Add noise\n",
    "\n",
    "# Split into train and test\n",
    "n_train = 30\n",
    "indices = np.random.permutation(n_total)\n",
    "train_idx, test_idx = indices[:n_train], indices[n_train:]\n",
    "\n",
    "x_train, y_train = x_all[train_idx], y_all[train_idx]\n",
    "x_test, y_test = x_all[test_idx], y_all[test_idx]\n",
    "\n",
    "# Evaluate different degrees\n",
    "degrees = range(1, 20)\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "for deg in degrees:\n",
    "    w = fit_polynomial(x_train, y_train, deg)\n",
    "    \n",
    "    # Training error\n",
    "    y_train_pred = predict_polynomial(x_train, w)\n",
    "    train_mse.append(np.mean((y_train - y_train_pred)**2))\n",
    "    \n",
    "    # Test error\n",
    "    y_test_pred = predict_polynomial(x_test, w)\n",
    "    test_mse.append(np.mean((y_test - y_test_pred)**2))\n",
    "\n",
    "# Plot the classic overfitting curve\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(degrees), y=train_mse, mode='lines+markers',\n",
    "                         name='Training Error', line=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=list(degrees), y=test_mse, mode='lines+markers',\n",
    "                         name='Test Error', line=dict(color='red')))\n",
    "\n",
    "# Find optimal degree\n",
    "best_deg = degrees[np.argmin(test_mse)]\n",
    "fig.add_vline(x=best_deg, line_dash='dash', line_color='green',\n",
    "              annotation_text=f'Best: degree {best_deg}')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='The Overfitting Curve: Training vs Test Error',\n",
    "    xaxis_title='Polynomial Degree',\n",
    "    yaxis_title='Mean Squared Error',\n",
    "    yaxis_type='log',\n",
    "    width=800, height=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"Best test performance at degree {best_deg}\")\n",
    "print(f\"  Training MSE: {train_mse[best_deg-1]:.4f}\")\n",
    "print(f\"  Test MSE: {test_mse[best_deg-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underfitting",
   "metadata": {},
   "source": [
    "## Underfitting: When Models Are Too Simple\n",
    "\n",
    "The opposite of overfitting is **underfitting**: the model is too simple to capture the underlying pattern.\n",
    "\n",
    "### Symptoms of Underfitting\n",
    "\n",
    "1. **High training error AND high test error**: Both are bad\n",
    "2. **Systematic patterns in residuals**: The model misses structure\n",
    "3. **Simple, smooth predictions**: May look \"reasonable\" but miss complexity\n",
    "\n",
    "### The Three Regimes\n",
    "\n",
    "| Regime | Training Error | Test Error | Diagnosis |\n",
    "|--------|---------------|------------|------------|\n",
    "| Underfitting | High | High | Model too simple |\n",
    "| Good fit | Low | Low | Model complexity appropriate |\n",
    "| Overfitting | Very low | High | Model too complex |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-regimes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three regimes\n",
    "fig = make_subplots(rows=1, cols=3, \n",
    "                    subplot_titles=('Underfitting (Degree 1)', \n",
    "                                   f'Good Fit (Degree {best_deg})', \n",
    "                                   'Overfitting (Degree 15)'))\n",
    "\n",
    "x_dense = np.linspace(-3.5, 3.5, 200)\n",
    "regimes = [1, best_deg, 15]\n",
    "\n",
    "for i, deg in enumerate(regimes, 1):\n",
    "    w = fit_polynomial(x_train, y_train, deg)\n",
    "    y_pred_dense = predict_polynomial(x_dense, w)\n",
    "    y_pred_dense = np.clip(y_pred_dense, -10, 10)  # Clip for visualization\n",
    "    \n",
    "    # Plot prediction\n",
    "    fig.add_trace(go.Scatter(x=x_dense, y=y_pred_dense, mode='lines',\n",
    "                             name=f'Degree {deg}', line=dict(color='red', width=2)),\n",
    "                  row=1, col=i)\n",
    "    \n",
    "    # Plot training data\n",
    "    fig.add_trace(go.Scatter(x=x_train, y=y_train, mode='markers',\n",
    "                             name='Train', marker=dict(size=8, color='blue')),\n",
    "                  row=1, col=i)\n",
    "    \n",
    "    # Plot test data\n",
    "    fig.add_trace(go.Scatter(x=x_test, y=y_test, mode='markers',\n",
    "                             name='Test', marker=dict(size=8, color='green', symbol='x')),\n",
    "                  row=1, col=i)\n",
    "    \n",
    "    # Compute errors\n",
    "    train_err = np.mean((y_train - predict_polynomial(x_train, w))**2)\n",
    "    test_err = np.mean((y_test - predict_polynomial(x_test, w))**2)\n",
    "    \n",
    "    fig.add_annotation(x=0, y=-8, text=f'Train: {train_err:.3f}<br>Test: {test_err:.3f}',\n",
    "                       showarrow=False, row=1, col=i)\n",
    "\n",
    "fig.update_yaxes(range=[-10, 10])\n",
    "fig.update_layout(width=1200, height=400, showlegend=False,\n",
    "                  title='Three Regimes: Underfitting, Good Fit, Overfitting')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weight-magnitudes",
   "metadata": {},
   "source": [
    "## Weight Magnitudes: A Window into Overfitting\n",
    "\n",
    "Overfitted models often have **extremely large weights**. This is because:\n",
    "\n",
    "1. To pass through noisy points exactly, the polynomial must oscillate rapidly\n",
    "2. Rapid oscillations require large positive and negative coefficients that nearly cancel\n",
    "3. Small changes in input cause large changes in output\n",
    "\n",
    "### The Polynomial Coefficient Explosion\n",
    "\n",
    "For a degree-$p$ polynomial with coefficients $w_0, w_1, \\ldots, w_p$:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\cdots + w_p x^p$$\n",
    "\n",
    "If $|w_i|$ are large and alternating in sign, the polynomial will oscillate wildly. This is a telltale sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weight-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weight magnitudes for different degrees\n",
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    subplot_titles=('Weight Magnitudes by Degree', 'Max |Weight| vs Degree'))\n",
    "\n",
    "degrees_to_show = [3, 7, 12, 17]\n",
    "colors = ['green', 'blue', 'orange', 'red']\n",
    "\n",
    "max_weights = []\n",
    "all_degrees = range(1, 20)\n",
    "\n",
    "for deg in all_degrees:\n",
    "    w = fit_polynomial(x_train, y_train, deg)\n",
    "    max_weights.append(np.max(np.abs(w)))\n",
    "\n",
    "# Left plot: weight values for selected degrees\n",
    "for deg, color in zip(degrees_to_show, colors):\n",
    "    w = fit_polynomial(x_train, y_train, deg)\n",
    "    fig.add_trace(go.Bar(x=[f'w{i}' for i in range(len(w))], y=w,\n",
    "                         name=f'Degree {deg}', marker_color=color, opacity=0.7),\n",
    "                  row=1, col=1)\n",
    "\n",
    "# Right plot: max weight vs degree\n",
    "fig.add_trace(go.Scatter(x=list(all_degrees), y=max_weights, mode='lines+markers',\n",
    "                         name='Max |weight|', line=dict(color='purple')),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text='Weight Value', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Max |Weight|', type='log', row=1, col=2)\n",
    "fig.update_xaxes(title_text='Coefficient', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Polynomial Degree', row=1, col=2)\n",
    "\n",
    "fig.update_layout(width=1000, height=400, \n",
    "                  title='Weight Explosion: A Signature of Overfitting')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nWeight magnitudes grow exponentially with degree!\")\n",
    "print(f\"Degree 3: max|w| = {max_weights[2]:.2f}\")\n",
    "print(f\"Degree 10: max|w| = {max_weights[9]:.2e}\")\n",
    "print(f\"Degree 15: max|w| = {max_weights[14]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-issues",
   "metadata": {},
   "source": [
    "## Numerical Stability: The Vandermonde Matrix Problem\n",
    "\n",
    "High-degree polynomial regression has serious numerical issues.\n",
    "\n",
    "### The Vandermonde Matrix\n",
    "\n",
    "The polynomial design matrix $\\Phi$ is called a **Vandermonde matrix**:\n",
    "\n",
    "$$\\Phi = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^p \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_n & x_n^2 & \\cdots & x_n^p \\end{bmatrix}$$\n",
    "\n",
    "### Why It's Ill-Conditioned\n",
    "\n",
    "1. **Columns become similar**: $x^{p-1}$ and $x^p$ are nearly collinear for large $p$\n",
    "2. **Huge value range**: If $x \\in [-3, 3]$, then $x^{15}$ ranges from $-14$ million to $+14$ million!\n",
    "3. **Poor condition number**: $\\kappa(\\Phi^T\\Phi)$ grows exponentially with degree\n",
    "\n",
    "### Consequences\n",
    "\n",
    "- $(\\Phi^T\\Phi)^{-1}$ becomes numerically unstable\n",
    "- Small changes in data cause huge changes in weights\n",
    "- Floating-point errors get amplified\n",
    "\n",
    "**Practical fix**: Use orthogonal polynomials (Legendre, Chebyshev) or regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "condition-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the ill-conditioning of Vandermonde matrices\n",
    "degrees = range(1, 20)\n",
    "condition_numbers = []\n",
    "\n",
    "for deg in degrees:\n",
    "    Phi = polynomial_features(x_train, deg)\n",
    "    # Condition number of Phi^T Phi\n",
    "    cond = np.linalg.cond(Phi.T @ Phi)\n",
    "    condition_numbers.append(cond)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(degrees), y=condition_numbers, mode='lines+markers',\n",
    "                         line=dict(color='red', width=2)))\n",
    "\n",
    "# Add reference line for machine epsilon\n",
    "machine_eps_inv = 1 / np.finfo(float).eps\n",
    "fig.add_hline(y=machine_eps_inv, line_dash='dash', line_color='gray',\n",
    "              annotation_text='~Machine precision limit')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Condition Number of Φ^TΦ vs Polynomial Degree',\n",
    "    xaxis_title='Polynomial Degree',\n",
    "    yaxis_title='Condition Number',\n",
    "    yaxis_type='log',\n",
    "    width=800, height=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"Condition numbers grow exponentially:\")\n",
    "for deg in [5, 10, 15]:\n",
    "    print(f\"  Degree {deg}: κ = {condition_numbers[deg-1]:.2e}\")\n",
    "print(f\"\\nMachine epsilon: {np.finfo(float).eps:.2e}\")\n",
    "print(f\"When κ > 1/eps ≈ {machine_eps_inv:.2e}, results become unreliable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generalization",
   "metadata": {},
   "source": [
    "## Generalization: The Ultimate Goal\n",
    "\n",
    "### What We Really Want\n",
    "\n",
    "The goal of machine learning is not to fit training data well, but to **generalize** to unseen data.\n",
    "\n",
    "**Generalization error** (or risk) is the expected error on new data:\n",
    "\n",
    "$$R(\\hat{f}) = \\mathbb{E}_{(x,y) \\sim P}[L(y, \\hat{f}(x))]$$\n",
    "\n",
    "where $P$ is the true data distribution and $L$ is our loss function.\n",
    "\n",
    "### The Generalization Gap\n",
    "\n",
    "$$\\text{Generalization Gap} = R(\\hat{f}) - \\hat{R}_{\\text{train}}(\\hat{f})$$\n",
    "\n",
    "where $\\hat{R}_{\\text{train}}$ is the training error.\n",
    "\n",
    "- **Small gap**: Model generalizes well\n",
    "- **Large gap**: Model overfits\n",
    "\n",
    "### Connection to Bias-Variance (Preview)\n",
    "\n",
    "The generalization error can be decomposed (covered in Section 6):\n",
    "\n",
    "$$\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}$$\n",
    "\n",
    "- High-degree polynomials: Low bias, high variance (overfitting)\n",
    "- Low-degree polynomials: High bias, low variance (underfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variance-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate variance in predictions: fit same model to different data samples\n",
    "np.random.seed(0)\n",
    "\n",
    "def generate_data(n, noise=0.5):\n",
    "    x = np.random.uniform(-3, 3, n)\n",
    "    y_true = np.sin(x) * 2 + 0.5 * x\n",
    "    y = y_true + np.random.randn(n) * noise\n",
    "    return x, y\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    subplot_titles=('Low Complexity (Degree 2): Low Variance',\n",
    "                                   'High Complexity (Degree 12): High Variance'))\n",
    "\n",
    "x_plot = np.linspace(-3.5, 3.5, 200)\n",
    "n_samples = 10\n",
    "degrees_compare = [2, 12]\n",
    "\n",
    "for col, deg in enumerate(degrees_compare, 1):\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        x_sample, y_sample = generate_data(30)\n",
    "        w = fit_polynomial(x_sample, y_sample, deg)\n",
    "        y_pred = predict_polynomial(x_plot, w)\n",
    "        y_pred = np.clip(y_pred, -8, 8)\n",
    "        all_predictions.append(y_pred)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=x_plot, y=y_pred, mode='lines',\n",
    "                                 line=dict(color='blue', width=1), opacity=0.3,\n",
    "                                 showlegend=False),\n",
    "                      row=1, col=col)\n",
    "    \n",
    "    # Plot mean prediction\n",
    "    mean_pred = np.mean(all_predictions, axis=0)\n",
    "    fig.add_trace(go.Scatter(x=x_plot, y=mean_pred, mode='lines',\n",
    "                             line=dict(color='red', width=3),\n",
    "                             name='Mean prediction'),\n",
    "                  row=1, col=col)\n",
    "    \n",
    "    # Plot true function\n",
    "    y_true_plot = np.sin(x_plot) * 2 + 0.5 * x_plot\n",
    "    fig.add_trace(go.Scatter(x=x_plot, y=y_true_plot, mode='lines',\n",
    "                             line=dict(color='black', width=2, dash='dash'),\n",
    "                             name='True function'),\n",
    "                  row=1, col=col)\n",
    "\n",
    "fig.update_yaxes(range=[-8, 8])\n",
    "fig.update_layout(width=1000, height=450,\n",
    "                  title='Variance in Predictions: Same Model, Different Training Sets')\n",
    "fig.show()\n",
    "\n",
    "print(\"Each blue line is the same model trained on a different random sample.\")\n",
    "print(\"High-complexity models vary wildly depending on the training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-tips",
   "metadata": {},
   "source": [
    "## Practical Considerations\n",
    "\n",
    "### When to Use Polynomial Features\n",
    "\n",
    "**Good candidates:**\n",
    "- Physical processes with known polynomial relationships\n",
    "- Interaction effects between features ($x_1 \\cdot x_2$)\n",
    "- Low-degree polynomials (2-4) for slight nonlinearity\n",
    "\n",
    "**Be careful with:**\n",
    "- High-degree polynomials (extrapolation is dangerous!)\n",
    "- Many features (combinatorial explosion of terms)\n",
    "- Unscaled data (numerical instability)\n",
    "\n",
    "### Alternatives to High-Degree Polynomials\n",
    "\n",
    "1. **Regularization** (Ridge/Lasso): Control weight magnitudes (Section 7)\n",
    "2. **Splines**: Piecewise polynomials with local support\n",
    "3. **Kernel methods**: Implicit high-dimensional features (SVMs)\n",
    "4. **Tree-based models**: Piecewise constant approximations\n",
    "5. **Neural networks**: Learned feature representations\n",
    "\n",
    "### Feature Scaling for Polynomials\n",
    "\n",
    "**Always scale features before creating polynomials!**\n",
    "\n",
    "If $x \\in [0, 1000]$, then $x^3 \\in [0, 10^9]$. This causes:\n",
    "- Numerical overflow\n",
    "- Ill-conditioned matrices\n",
    "- Gradient descent convergence issues\n",
    "\n",
    "Standard approach: Standardize first, then create polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scaling-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the importance of scaling\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data with large range\n",
    "x_large = np.random.uniform(0, 100, 30)\n",
    "y_large = 0.001 * x_large**2 - 0.1 * x_large + 5 + np.random.randn(30) * 2\n",
    "\n",
    "# Without scaling\n",
    "Phi_unscaled = polynomial_features(x_large, degree=5)\n",
    "cond_unscaled = np.linalg.cond(Phi_unscaled.T @ Phi_unscaled)\n",
    "\n",
    "# With scaling (standardization)\n",
    "x_scaled = (x_large - x_large.mean()) / x_large.std()\n",
    "Phi_scaled = polynomial_features(x_scaled, degree=5)\n",
    "cond_scaled = np.linalg.cond(Phi_scaled.T @ Phi_scaled)\n",
    "\n",
    "print(\"Effect of Scaling on Numerical Stability\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Original x range: [{x_large.min():.1f}, {x_large.max():.1f}]\")\n",
    "print(f\"Scaled x range:   [{x_scaled.min():.2f}, {x_scaled.max():.2f}]\")\n",
    "print(f\"\\nCondition number (degree 5):\")\n",
    "print(f\"  Unscaled: {cond_unscaled:.2e}\")\n",
    "print(f\"  Scaled:   {cond_scaled:.2e}\")\n",
    "print(f\"  Improvement: {cond_unscaled/cond_scaled:.0f}x better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Polynomial features** extend linear models to capture nonlinear patterns\n",
    "   - Transform: $x \\rightarrow [1, x, x^2, \\ldots, x^p]$\n",
    "   - Still linear in parameters, nonlinear in inputs\n",
    "\n",
    "2. **Model capacity** increases with polynomial degree\n",
    "   - More parameters = more flexibility\n",
    "   - Degree $n-1$ can interpolate $n$ points exactly\n",
    "\n",
    "3. **Overfitting** occurs when capacity exceeds what data supports\n",
    "   - Symptoms: Low training error, high test error, large weights\n",
    "   - Model learns noise, not just signal\n",
    "\n",
    "4. **Underfitting** occurs when capacity is insufficient\n",
    "   - Symptoms: High training AND test error\n",
    "   - Model misses important patterns\n",
    "\n",
    "5. **Numerical issues** plague high-degree polynomials\n",
    "   - Vandermonde matrices are ill-conditioned\n",
    "   - Always scale features before polynomial expansion\n",
    "\n",
    "### Connections to Other Topics\n",
    "\n",
    "- **Section 4** (Train/Val/Test): How to properly evaluate generalization\n",
    "- **Section 5** (Feature Scaling): Why and how to scale before polynomial features\n",
    "- **Section 6** (Bias-Variance): Formal framework for the overfitting tradeoff\n",
    "- **Section 7** (Regularization): How to control overfitting mathematically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
